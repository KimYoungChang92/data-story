loss(theta0)
x %*% theta0
dim(x)
dim(theta0)
theta0 <- t(rep(0, ncol))
dim(theta0)
theta0 <- (rep(0, ncol))
dim(theta0)
theta0 <- matrix(rep(0, ncol),10,1)
theta0
loss(theta0)
x %*% theta0
dim(x)
dim(theta0)
x
y <- as.matrix(y)
x <- as.matrix(x)
loss(theta)
loss(theta0)
loss <- function(theta) 0.5*sum(y-x %*% theta)**2 / length(y)
loss(theta0)
grad <- function(theta) t(theta) %*% t(y-x %*% theta) / length(y)
grad(theta0)
dim(y)
y <- as.matrix(y,ncol =1)
dim(y)
dim(theta0)
dim(x)
grad <- function(theta) t(x) %*% t(y-x %*% theta) / length(y)
grad(theta0)
x %*% theta0
y - x %*% theta0
dim(x)
dim(t(x))
grad <- function(theta) t(x) %*% (y-x %*% theta) / length(y)
grad(theta0)
lambda <- 0.5
loss <- function(theta) 0.5*sum(y-x %*% theta)**2 / length(y) + 0.5*lambda*theta**2
loss(theta0)
loss <- function(theta) 0.5*sum(y-x %*% theta)**2 / length(y) + 0.5*sum(lambda*theta**2) / length(theta)
loss(theta0)
theta0 <- matrix(rep(0.5, ncol),10,1)
loss(theta0)
grad <- function(theta) t(x) %*% (y-x %*% theta) / length(y) + lambda * sum(abs(theta))
grad(theta0)
alpha <- 0.1
loss0 <- loss(theta0)
grad0 <- grad(theta0)
loss0
theta1 <- theta0 - alpha*grad(theta0)
loss(theta1)
theta1 <- theta0 + alpha*grad(theta0)
loss(theta1)
grad <- function(theta) t(x) %*% (y-x %*% theta) / length(y) + lambda * sum(abs(theta))/ length(theta)
loss0 <- loss(theta0)
grad0 <- grad(theta0)
theta1 <- theta0 - alpha*grad(theta0)
loss0
loss1
loss(theta1)
grad0
theta0
alpha <- 0.001
theta1 <- theta0 - alpha*grad(theta0)
loss(theta1)
sum(abs(theta0))
sum(abs(theta0))/length(theta0)
x
y
theta0 <- matrix(rep(20, ncol),10,1)
loss0 <- loss(theta0)
grad0 <- grad(theta0)
grad0
loss0
theta1 <- theta0 - alpha*grad(theta0)
loss1 <- loss(theta1)
theta1 <- theta0 + alpha*grad(theta0)
loss1 <- loss(theta1)
grad <- function(theta) -t(x) %*% (y-x %*% theta) / length(y) + lambda * sum(abs(theta))/ length(theta)
loss0 <- loss(theta0)
grad0 <- grad(theta0)
theta1 <- theta0 + alpha*grad(theta0)
loss1 <- loss(theta1)
alpha <- 0.1
theta1 <- theta0 + alpha*grad(theta0)
loss1 <- loss(theta1)
theta1 <- theta0 - alpha*grad(theta0)
loss1 <- loss(theta1)
loss <- function(theta) 0.5*sum((y-x %*% theta)**2) / length(y) + 0.5*sum(lambda*theta**2) / length(theta)
grad <- function(theta) -t(x) %*% (y-x %*% theta) / length(y) + lambda * sum(abs(theta))/ length(theta)
loss0 <- loss(theta0)
grad0 <- grad(theta0)
theta1 <- theta0 - alpha*grad(theta0)
loss1 <- loss(theta1)
loss <- function(theta) 0.5*sum((y-x %*% theta)**2) / length(y) + 0.5*lambda*sum(theta**2) / length(theta)
grad <- function(theta) -t(x) %*% (y-x %*% theta) / length(y) + lambda * sum(abs(theta))/ length(theta)
loss0 <- loss(theta0)
theta0 <- matrix(rep(0, ncol),10,1)
lambda <- 0.5
alpha <- 0.1
loss <- function(theta) 0.5*sum((y-x %*% theta)**2) / length(y) + 0.5*lambda*sum(theta**2) / length(theta)
grad <- function(theta) -t(x) %*% (y-x %*% theta) / length(y) + lambda * sum(abs(theta))/ length(theta)
loss0 <- loss(theta0)
grad0 <- grad(theta0)
theta1 <- theta0 - alpha*grad(theta0)
loss1 <- loss(theta1)
cbind(theta0,theta1)
grad(theta0)
loop <- function(theta){
l <- loss(theta)
g <- grad(theta)
theta <- theta  - alpha*g
return(list(loss = l, grad = g, theta = theta))
}
loop(theta0)
for (i in 1:5) theta0 <- theta0 - alpha*grad(theta0);print (loss(theta0))
for (i in 1:5) {
theta0 <- theta0 - alpha*grad(theta0)
print (loss(theta0))}
for (i in 1:5) {
theta0 <- theta0 + alpha*grad(theta0)
print (loss(theta0))}
theta0 <- matrix(rep(0, ncol),10,1)
lambda <- 0.5
alpha <- 0.1
loss <- function(theta) 0.5*sum((y-x %*% theta)**2)
grad <- function(theta) -t(x) %*% (y-x %*% theta)
for (i in 1:5) {
theta0 <- theta0 + alpha*grad(theta0)
print (loss(theta0))}
theta0 <- matrix(rep(0, ncol),10,1)
for (i in 1:5) {
theta0 <- theta0 - alpha*grad(theta0)
print (loss(theta0))}
alpha <- 0.0001
for (i in 1:5) {
theta0 <- theta0 - alpha*grad(theta0)
print (loss(theta0))}
theta0 <- matrix(rep(0, ncol),10,1)
for (i in 1:5) {
theta0 <- theta0 - alpha*grad(theta0)
print(theta0)
print (loss(theta0))}
nrow
grad <- function(theta) t(x) %*% (y-x %*% theta)/nrow
for (i in 1:5) {
theta0 <- theta0 - alpha*grad(theta0)
print(theta0)
print (loss(theta0))}
theta0 <- matrix(rep(0, ncol),10,1)
for (i in 1:5) {
theta0 <- theta0 - alpha*grad(theta0)
print (loss(theta0))}
grad <- function(theta) t(x) %*% (y-x %*% theta)
theta0 <- matrix(rep(0, ncol),10,1)
for (i in 1:5) {
theta0 <- theta0 - alpha*grad(theta0)
print (loss(theta0))}
alpha <- 100
theta0 <- matrix(rep(0, ncol),10,1)
for (i in 1:5) {
theta0 <- theta0 - alpha*grad(theta0)
print (loss(theta0))}
alpha <- 0.5
theta0 <- matrix(rep(0, ncol),10,1)
for (i in 1:5) {
theta0 <- theta0 - alpha*grad(theta0)
print (loss(theta0))}
loss <- function(theta) 0.5*sum((y-x %*% theta)**2) /nrow
grad <- function(theta) t(x) %*% (y-x %*% theta) /nrow
theta0 <- matrix(rep(0, ncol),10,1)
for (i in 1:5) {
theta0 <- theta0 - alpha*grad(theta0)
print (loss(theta0))}
theta0 <- matrix(rep(0, ncol),10,1)
for (i in 1:5) {
theta0 <- theta0 + alpha*grad(theta0)
print (loss(theta0))}
loss <- function(x) x**3+x+1
plot(seq(0,1,.01), loss(seq(0,1,0.01)))
plot(seq(-1,1,.01), loss(seq(-1,1,0.01)))
loss <- function(x) x**2+x+1
plot(seq(-1,1,.01), loss(seq(-1,1,0.01)))
grad <- function(x) 2*x+1
x0 <- 0
x0 <- 0
for(i in 1:5){
x0 <- x0 + 0.1*grad(x0)
print(loss(x0))
}
x0 <- 0
for(i in 1:5){
x0 <- x0 - 0.1*grad(x0)
print(loss(x0))
}
x0 <- 0
for(i in 1:5){
x0 <- x0 - 0.5*grad(x0)
print(loss(x0))
}
x0 <- 0
for(i in 1:5){
x0 <- x0 - 0.8*grad(x0)
print(loss(x0))
}
x0 <- 0
for(i in 1:10){
x0 <- x0 - 0.8*grad(x0)
print(loss(x0))
}
?lm
setwd("~/Desktop/github/DataStory/RecommenderSystem/ml-latest-small")
rm(list = ls())
gc()
library(data.table)
library(dplyr)
library(tidyr)
library(reshape2)
library(Metrics)
schema <- c("userId" = "integer",
"movieId" = "integer",
"rating" = "numeric",
"timestamp" = "character")
df <- fread("ratings.csv", colClasses = schema)
## label train (+1) / val (-1)
df$label <- 1
set.seed(2333)
df$label[sample(1:nrow(df),0.3*nrow(df))] <- (-1)
# Y = U X I
Y = dcast(df, userId ~ movieId, value.var = 'rating', fill = 0)
Y <- as.matrix(Y[2:ncol(Y)])
R <- dcast(df,userId ~ movieId, value.var = 'label', fill = 0)
R <- as.matrix(R[2:ncol(R)])
R.train <- R
R.train[R.train<1] <- 0
R.val <- (-R)
R.val[R.val<1] <- 0
## loss function - minimal square error
loss <- function(Y,U,I,R,lambda){
return((sum(R*(Y-U %*% t(I))**2) + lambda*sum(U**2) + lambda*sum(I**2))/sum(R)/2)
}
## gradient function - user matrix
grad_u <- function(Y,U,I,R,lambda) (-(R*(Y - U%*%t(I))) %*% I + lambda*abs(U))/sum(R)
## gradient function - item matrix
grad_i <- function(Y,U,I,R,lambda) (-t(R*(Y - U%*%t(I))) %*% U + lambda*abs(I))/sum(R)
## gradient descent optimizer
gd <- function(Y,U,I,R, lambda, alpha, maxIter,thresh = 1e-4){
loss0 <- loss(Y,U,I,R[[1]],lambda)
alpha0 <- alpha
gr_u <- grad_u(Y,U,I,R[[1]],lambda)
gr_i <- grad_i(Y,U,I,R[[1]],lambda)
for(i in 1:maxIter){
## update user,item matrix simutaneously
U <- U - gr_u * alpha
I <- I - gr_i * alpha
loss1 <- loss(Y,U,I,R[[1]],lambda)
## check the train/validation error
pred <- (U%*%t(I))
rmse.train <- rmse(pred[R[[1]]>0],Y[R[[1]]>0])
rmse.val <- rmse(pred[R[[2]]>0],Y[R[[2]]>0])
if (abs(loss1-loss0)<thresh | loss1 > loss0) break
cat('iter',i,': loss',loss1,'alpha',alpha,'rmse.train',rmse.train,
'rmse.val',rmse.val,'\n')
loss0 <- loss1
## update gradients
gr_u <- grad_u(Y,U,I,R[[1]],lambda)
gr_i <- grad_i(Y,U,I,R[[1]],lambda)
}
return(list(U = U,I = I,loss = loss1,iter = i, rmse.train = rmse.train, rmse.val = rmse.val))
}
## alternate least square optimizer
als <- function(Y,U,I,R,lambda, alpha, maxIter,thresh = 1e-4){
loss0 <- loss(Y,U,I,R[[1]],lambda)
alpha0 <- alpha
gr_u <- grad_u(Y,U,I,R[[1]],lambda)
gr_i <- grad_i(Y,U,I,R[[1]],lambda)
for(i in 1:maxIter){
# update User Matrix
U <- U - gr_u * alpha
loss1 <- loss(Y,U,I,R[[1]],lambda)
pred <- (U%*%t(I))
rmse.train <- rmse(pred[R[[1]]>0],Y[R[[1]]>0])
rmse.val <- rmse(pred[R[[2]]>0],Y[R[[2]]>0])
if (abs(loss1-loss0)<thresh | loss1 > loss0) break
cat('U iter',i,': loss',loss1,'alpha',alpha,'rmse.train',rmse.train,
'rmse.val',rmse.val,'\n')
gr_u <- grad_u(Y,U,I,R[[1]],lambda)
loss0 <- loss1
# update Item Matrix
I <- I - gr_i * alpha
loss1 <- loss(Y,U,I,R[[1]],lambda)
pred <- (U%*%t(I))
rmse.train <- rmse(pred[R[[1]]>0],Y[R[[1]]>0])
rmse.val <- rmse(pred[R[[2]]>0],Y[R[[2]]>0])
if (abs(loss1-loss0)<thresh | loss1 > loss0) break
cat('I iter',i,': loss',loss1,'alpha',alpha,'rmse.train',rmse.train,
'rmse.val',rmse.val,'\n')
gr_i <- grad_i(Y,U,I,R[[1]],lambda)
loss0 <- loss1
}
return(list(U = U,I = I,loss = loss1,iter = i, rmse.train = rmse.train, rmse.val = rmse.val))
}
dim(Y)
load("model_result")
ls
ls()
load('model_result')
setwd("~/Desktop/github/RecommenderSystem/ml-latest-small")
source('~/Desktop/github/DataStory/RecommenderSystem/ml-latest-small/model_based_recsys.R')
## loss function - minimal square error
loss <- function(Y,U,I,R,lambda){
return((sum(R*(Y-U %*% t(I))**2) + lambda*sum(U**2) + lambda*sum(I**2))/sum(R)/2)
}
## gradient function - user matrix
grad_u <- function(Y,U,I,R,lambda) (-(R*(Y - U%*%t(I))) %*% I + lambda*abs(U))/sum(R)
## gradient function - item matrix
grad_i <- function(Y,U,I,R,lambda) (-t(R*(Y - U%*%t(I))) %*% U + lambda*abs(I))/sum(R)
## gradient descent optimizer
gd <- function(Y,U,I,R, lambda, alpha, maxIter,thresh = 1e-4){
loss0 <- loss(Y,U,I,R[[1]],lambda)
alpha0 <- alpha
pred0 <- (U%*%t(I))
rmse.train0 <- rmse(pred[R[[1]]>0],Y[R[[1]]>0])
rmse.val0 <- rmse(pred[R[[2]]>0],Y[R[[2]]>0])
gr_u <- grad_u(Y,U,I,R[[1]],lambda)
gr_i <- grad_i(Y,U,I,R[[1]],lambda)
rmse.train.history <- NULL
rmse.val.history <- NULL
for(i in 1:maxIter){
## update user,item matrix simutaneously
U <- U - gr_u * alpha
I <- I - gr_i * alpha
loss1 <- loss(Y,U,I,R[[1]],lambda)
## check the train/validation error
pred <- (U%*%t(I))
rmse.train <- rmse(pred[R[[1]]>0],Y[R[[1]]>0])
rmse.val <- rmse(pred[R[[2]]>0],Y[R[[2]]>0])
## record rmse history
rmse.train.history <- c(rmse.train.history, rmse.train)
rmse.val.history <- c(rmse.val.history,rmse.val)
## stop citeria: increasing loss, validation rmse
## or the difference of loss is less than treshold
if (abs(loss1-loss0)<thresh | loss1 > loss0 | rmse.val > rmse.val0) break
cat('iter',i,': loss',loss1,'alpha',alpha,'rmse.train',rmse.train,
'rmse.val',rmse.val,'\n')
# update the starting point
loss0 <- loss1
pred0 <- pred
rmse.train0 <- rmse.train
rmse.val0 <- rmse.val
## update gradients
gr_u <- grad_u(Y,U,I,R[[1]],lambda)
gr_i <- grad_i(Y,U,I,R[[1]],lambda)
}
rmse.history <- data.frame(iter = 1:i, rmse.train = rmse.train.history, rmse.val = rmse.val.history)
return(list(U = U,I = I,loss = loss1,rmse.history = rmse.history))
}
gd.time <- system.time(res.gd <- gd(Y,U,I,list(R.train,R.val),0.3,30,10)) # 70s, rmse:0.946
gd <- function(Y,U,I,R, lambda, alpha, maxIter,thresh = 1e-4){
loss0 <- loss(Y,U,I,R[[1]],lambda)
alpha0 <- alpha
pred0 <- (U%*%t(I))
rmse.train0 <- rmse(pred0[R[[1]]>0],Y[R[[1]]>0])
rmse.val0 <- rmse(pred0[R[[2]]>0],Y[R[[2]]>0])
gr_u <- grad_u(Y,U,I,R[[1]],lambda)
gr_i <- grad_i(Y,U,I,R[[1]],lambda)
rmse.train.history <- NULL
rmse.val.history <- NULL
for(i in 1:maxIter){
## update user,item matrix simutaneously
U <- U - gr_u * alpha
I <- I - gr_i * alpha
loss1 <- loss(Y,U,I,R[[1]],lambda)
## check the train/validation error
pred <- (U%*%t(I))
rmse.train <- rmse(pred[R[[1]]>0],Y[R[[1]]>0])
rmse.val <- rmse(pred[R[[2]]>0],Y[R[[2]]>0])
## record rmse history
rmse.train.history <- c(rmse.train.history, rmse.train)
rmse.val.history <- c(rmse.val.history,rmse.val)
## stop citeria: increasing loss, validation rmse
## or the difference of loss is less than treshold
if (abs(loss1-loss0)<thresh | loss1 > loss0 | rmse.val > rmse.val0) break
cat('iter',i,': loss',loss1,'alpha',alpha,'rmse.train',rmse.train,
'rmse.val',rmse.val,'\n')
# update the starting point
loss0 <- loss1
pred0 <- pred
rmse.train0 <- rmse.train
rmse.val0 <- rmse.val
## update gradients
gr_u <- grad_u(Y,U,I,R[[1]],lambda)
gr_i <- grad_i(Y,U,I,R[[1]],lambda)
}
rmse.history <- data.frame(iter = 1:i, rmse.train = rmse.train.history, rmse.val = rmse.val.history)
return(list(U = U,I = I,loss = loss1,rmse.history = rmse.history))
}
gd.time <- system.time(res.gd <- gd(Y,U,I,list(R.train,R.val),0.3,30,10)) # 70s, rmse:0.946
res.gd$rmse.history
library(ggplot2)
ggplot(res.gd$rmse.history) + geom_line(aes(x = iter, y = rmse.train), colour = 'red') +geom_line(aes(x = iter, y = rmse.val), colour = 'green')
## alternate least square optimizer
als <- function(Y,U,I,R,lambda, alpha, maxIter,thresh = 1e-4){
loss0 <- loss(Y,U,I,R[[1]],lambda)
alpha0 <- alpha
pred0 <- (U%*%t(I))
rmse.train0 <- rmse(pred0[R[[1]]>0],Y[R[[1]]>0])
rmse.val0 <- rmse(pred0[R[[2]]>0],Y[R[[2]]>0])
gr_u <- grad_u(Y,U,I,R[[1]],lambda)
gr_i <- grad_i(Y,U,I,R[[1]],lambda)
# rmse history
rmse.train.history <- NULL
rmse.val.history <- NULL
iter <- 0
for(i in 1:maxIter){
# update User Matrix
U <- U - gr_u * alpha
loss1 <- loss(Y,U,I,R[[1]],lambda)
pred <- (U%*%t(I))
rmse.train <- rmse(pred[R[[1]]>0],Y[R[[1]]>0])
rmse.val <- rmse(pred[R[[2]]>0],Y[R[[2]]>0])
if (abs(loss1-loss0)<thresh | loss1 > loss0 | rmse.val > rmse.val0) break
cat('U iter',i,': loss',loss1,'alpha',alpha,'rmse.train',rmse.train,
'rmse.val',rmse.val,'\n')
# record rmse history
rmse.train.history <- c(rmse.train.history, rmse.train)
rmse.val.history <- c(rmse.val.history, rmse.val)
iter <- iter + 1
# update the gradient
gr_u <- grad_u(Y,U,I,R[[1]],lambda)
# update the starting point
loss0 <- loss1
pred0 <- pred
rmse.train0 <- rmse.train
rmse.val0 <- rmse.val
# update Item Matrix
I <- I - gr_i * alpha
loss1 <- loss(Y,U,I,R[[1]],lambda)
pred <- (U%*%t(I))
rmse.train <- rmse(pred[R[[1]]>0],Y[R[[1]]>0])
rmse.val <- rmse(pred[R[[2]]>0],Y[R[[2]]>0])
if (abs(loss1-loss0)<thresh | loss1 > loss0 | rmse.val > rmse.val0) break
cat('I iter',i,': loss',loss1,'alpha',alpha,'rmse.train',rmse.train,
'rmse.val',rmse.val,'\n')
# record rmse history
rmse.train.history <- c(rmse.train.history, rmse.train)
rmse.val.history <- c(rmse.val.history, rmse.val)
iter <- iter + 1
# update the gradient
gr_i <- grad_i(Y,U,I,R[[1]],lambda)
# update the starting point
loss0 <- loss1
pred0 <- pred
rmse.train0 <- rmse.train
rmse.val0 <- rmse.val
}
rmse.history <- data.frame(iter = 1:iter, rmse.train = rmse.train.history, rmse.val = rmse.val.history)
return(list(U = U,I = I,loss = loss1,rmse.history = rmse.history))
}
als.time <- system.time(res.als <- als(Y,U,I,list(R.train,R.val),0.3,30,5)) #46s,rmse:0.994
res.als$rmse.history
gd.time <- system.time(res.gd <- gd(Y,U,I,list(R.train,R.val),0.3,30,1000)) # 70s, rmse:0.946
als.time <- system.time(res.als <- als(Y,U,I,list(R.train,R.val),0.3,30,500)) #46s,rmse:0.994
## save model result
save(gd.time,als.time,res.gd, file = 'model_result')
gd <- melt(res.gd$rmse.history)
names(gd)
View(gd)
gd <- melt(res.gd$rmse.history, id.vars = 'iter')
als <- melt(res.als$rmse.history, id.vars = "iter")
gd$method = 'gradient descent'
als$method = 'als'
history <- rbind(gd,als)
View(history)
history$method <- paste(history$method, history$variable)
ggplot(history,aes(x = iter, y = value,colour = factor(method)))+geom_point()+geom_line()
ggplot(history,aes(x = iter, y = value,colour = factor(method)))+geom_point(alpha = 0.1)+geom_line()
ggplot(history,aes(x = iter, y = value,colour = factor(method)))+geom_point(alpha = 0.5)+geom_line()
ggplot(history,aes(x = iter, y = value,colour = factor(method)))+geom_point(alpha = 0.2)+geom_line()
tail(res.gd$rmse.history,1)
tail(res.als$rmse.history,1)
als.time
gd.time
library(ggplot2)
gd <- melt(res.gd$rmse.history, id.vars = 'iter')
als <- melt(res.als$rmse.history, id.vars = "iter")
gd$method = 'gradient descent'
als$method = 'alternating least square'
history <- rbind(gd,als)
history$method <- paste(history$method, history$variable)
ggplot(history,aes(x = iter, y = value,colour = factor(method)))+geom_point(alpha = 0.2)+geom_line()
## plot the rmse hisotry
library(ggplot2)
gd <- melt(res.gd$rmse.history, id.vars = 'iter')
als <- melt(res.als$rmse.history, id.vars = "iter")
gd$method = 'gradient descent'
als$method = 'alternating least square'
history <- rbind(gd,als)
history$method <- paste(history$method, history$variable)
ggplot(history,aes(x = iter, y = value,colour = factor(method)))+geom_point(alpha = 0.2)+geom_line()+
labs(y = 'rmse')
tail(res.gd$rmse.history,1)
tail(res.als$rmse.history,1)
gd.time
als.time
hist.plot <- ggplot(history,aes(x = iter, y = value,colour = factor(method)))+geom_point(alpha = 0.2)+geom_line()+
labs(y = 'rmse')
render("report.Rmd")
library(rmarkdown)
render("report.Rmd")
setwd("~/Desktop/github/DataStory/RecommenderSystem/ml-latest-small")
render("report.Rmd")
